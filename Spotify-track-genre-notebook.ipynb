{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-1OspiNHvp9"
      },
      "source": [
        "# <font color =\"#0B6FA6\">Capítulo da Sociedade de Inteligência Computacional do IEEE UFPB </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj7tEdXcNzq_"
      },
      "source": [
        "## Preencha seus dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Oq_Q-p3iOLZO"
      },
      "outputs": [],
      "source": [
        "#@title Preencha os campos abaixo:\n",
        "\n",
        "nome = 'Laura de Faria' #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlbA-wBCN3rd"
      },
      "source": [
        "## Aprendizagem de Máquina\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurjGSMaYqBY"
      },
      "source": [
        "Para exercitar os conceitos de aprendizagem de máquina, utiliza-se do dataset \"Spotify track genre\" para prever o gênero de uma música. Os dados recebidos já estão limpos, para mais informações sobre, visite a página [aqui](https://www.kaggle.com/datasets/thedevastator/spotify-tracks-genre-dataset/data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZjjfG-KgiRv"
      },
      "source": [
        "A seguir, o dataframe já estará montado pelo método `pd.read_csv()` da biblioteca `pandas`, a partir da execução da célula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIpc20ZlgI08",
        "outputId": "fe3ed59a-2725-4a6f-ebc6-27226aa6a8c8"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'trackgenre.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrackgenre.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Dimensão do dataset (Linhas x Colunas)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
            "File \u001b[1;32mc:\\Users\\lauraa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\lauraa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\lauraa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\lauraa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\lauraa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trackgenre.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('trackgenre.csv')\n",
        "\n",
        "# Dimensão do dataset (Linhas x Colunas)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
        "                           f1_score, silhouette_score, calinski_harabasz_score)\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-QQPwlVW_TX"
      },
      "source": [
        "### 1. Atributos de entrada e saída\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZVbD90gXE53"
      },
      "source": [
        "Em um contexto de modelagem de dados, os atributos de entrada são as informações utilizadas para fazer previsões ou classificações, enquanto o atributo de saída é a variável que se pretende prever ou classificar. Considere que o atributo de saída será o gênero musical, sendo identificado pela coluna `'track_genre'`. Os atributos de entrada, incluem as características relevantes ou informações que serão usadas pelo modelo para fazer suas previsões ou classificações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define target variable (output)\n",
        "target_column = 'track_genre'\n",
        "print(f\"Target variable (output): {target_column}\")\n",
        "print(f\"Number of unique genres: {df[target_column].nunique()}\")\n",
        "print(f\"Genres: {sorted(df[target_column].unique())}\")\n",
        "\n",
        "# Define input features (exclude non-numeric and target columns)\n",
        "exclude_columns = ['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', 'track_genre']\n",
        "input_features = [col for col in df.columns if col not in exclude_columns]\n",
        "\n",
        "print(f\"\\nInput features ({len(input_features)}):\")\n",
        "for i, feature in enumerate(input_features, 1):\n",
        "    print(f\"{i:2d}. {feature}\")\n",
        "\n",
        "# Create feature matrix X and target vector y\n",
        "X = df[input_features].copy()\n",
        "y = df[target_column].copy()\n",
        "\n",
        "print(f\"\\nFeature matrix X shape: {X.shape}\")\n",
        "print(f\"Target vector y shape: {y.shape}\")\n",
        "\n",
        "# Handle any remaining non-numeric columns\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
        "print(f\"Categorical features: {len(categorical_features)}\")\n",
        "\n",
        "# Encode categorical features if any\n",
        "if categorical_features:\n",
        "    print(f\"Encoding categorical features: {categorical_features}\")\n",
        "    le = LabelEncoder()\n",
        "    for col in categorical_features:\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "\n",
        "print(f\"Final input features shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Es_kHwXxvH"
      },
      "source": [
        "### 2. Amostragem\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfuxyq1pX9Pe"
      },
      "source": [
        "Na divisão em conjuntos de teste e treino, a proporção decidida será de 80% para o conjunto de treino e 20% para o conjunto de teste. Essa divisão é realizada baseada na estratificação. Nisso, sem ela pode haver uma distribuição desigual das classes nos conjuntos de treinamento e teste, o que pode levar a um modelo tendencioso ou com desempenho insatisfatório."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]:,} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "# Check stratification worked properly\n",
        "print(\"\\nGenre distribution in training set:\")\n",
        "train_distribution = y_train.value_counts(normalize=True).sort_index()\n",
        "print(train_distribution)\n",
        "\n",
        "print(\"\\nGenre distribution in test set:\")\n",
        "test_distribution = y_test.value_counts(normalize=True).sort_index()\n",
        "print(test_distribution)\n",
        "\n",
        "# Visualize the distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "train_distribution.plot(kind='bar', ax=ax1, color='skyblue')\n",
        "ax1.set_title('Genre Distribution - Training Set')\n",
        "ax1.set_xlabel('Genre')\n",
        "ax1.set_ylabel('Proportion')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "test_distribution.plot(kind='bar', ax=ax2, color='lightcoral')\n",
        "ax2.set_title('Genre Distribution - Test Set')\n",
        "ax2.set_xlabel('Genre')\n",
        "ax2.set_ylabel('Proportion')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLgLgYRYBRG"
      },
      "source": [
        "### 3. Pipeline para redução da dimensionalidade dos dados\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsRTognYJeH"
      },
      "source": [
        "Na etapa de pré-processamento dos dados, é essencial considerar técnicas de redução da dimensionalidade para simplificar o conjunto de dados sem perder informações relevantes. Duas técnicas amplamente utilizadas para essa finalidade são o PCA e o t-SNE. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfk9t9zpdaAO"
      },
      "source": [
        "`t-SNE` (t-Distributed Stochastic Neighbor Embedding) é um algoritmo de redução de dimensionalidade não linear. O objetivo do t-SNE é representar dados de alta dimensão em um espaço de baixa dimensão (geralmente 2D ou 3D) de uma maneira que preserva as relações e a estrutura dos dados originais.\n",
        "\n",
        "O `PCA` (Análise de Componentes Principais) é uma técnica estatística que transforma um conjunto de variáveis possivelmente correlacionadas em um conjunto de valores de variáveis linearmente não correlacionadas, chamadas de componentes principais. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesta seção, você precisará implementar a redução de dimensionalidade utilizando as duas técnicas mencionadas: PCA e t-SNE. Isso permitirá que você compare os resultados e escolha a abordagem que melhor se adapta aos seus dados, utilizando-a na próxima seção."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features for dimensionality reduction\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"3.1 PCA (Principal Component Analysis)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "# Visualize PCA results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# Sample data for visualization (too many points would be cluttered)\n",
        "sample_idx = np.random.choice(len(X_train_pca), size=5000, replace=False)\n",
        "scatter = plt.scatter(X_train_pca[sample_idx, 0], X_train_pca[sample_idx, 1], \n",
        "                     c=pd.Categorical(y_train.iloc[sample_idx]).codes, \n",
        "                     alpha=0.6, s=1, cmap='tab20')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
        "plt.title('PCA - 2D Visualization of Genres')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "print(\"\\n3.2 t-SNE (t-Distributed Stochastic Neighbor Embedding)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Apply t-SNE (on a subset due to computational complexity)\n",
        "sample_size = min(10000, len(X_train_scaled))\n",
        "sample_idx_tsne = np.random.choice(len(X_train_scaled), size=sample_size, replace=False)\n",
        "X_sample = X_train_scaled[sample_idx_tsne]\n",
        "y_sample = y_train.iloc[sample_idx_tsne]\n",
        "\n",
        "print(f\"Applying t-SNE on {sample_size} samples...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_sample)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
        "                     c=pd.Categorical(y_sample).codes, \n",
        "                     alpha=0.6, s=1, cmap='tab20')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE - 2D Visualization of Genres')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Choose PCA for further analysis due to computational efficiency\n",
        "print(\"\\nFor subsequent analysis, we'll use PCA-reduced data for better performance.\")\n",
        "X_train_reduced = X_train_pca\n",
        "X_test_reduced = X_test_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwLx3DNbZHGm"
      },
      "source": [
        "### 4 Aprendizagem Supervisionada\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz06ar5-ZTJ1"
      },
      "source": [
        "Para a classificação, iremos utilizar os modelos `Decision Tree`, `KNN` e `Random Forest`. Como indicadores de qualidade de prformance, deve-se analisar métricas como:\n",
        "\n",
        "- **Matriz de Confusão**: Uma tabela que permite visualizar o desempenho de um modelo de classificação em termos de previsões corretas e incorretas. Ela mostra a contagem de verdadeiros positivos (TP), verdadeiros negativos (TN), falsos positivos (FP) e falsos negativos (FN) para cada classe do problema.\n",
        "- **Acurácia**: A acurácia é uma métrica comumente usada para avaliar a precisão de um modelo de classificação. Ela mede a proporção de exemplos classificados corretamente em relação ao total de exemplos.\n",
        "- **F1 Score**: É uma medida que combina a precisão (precision) e a revocação (recall) em um único valor, fornecendo uma visão geral do desempenho do modelo. A precisão representa a proporção de exemplos classificados corretamente como positivos em relação ao total de exemplos classificados como positivos. A revocação, por sua vez, representa a proporção de exemplos classificados corretamente como positivos em relação ao total de exemplos que são realmente positivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_final = X_train_scaled\n",
        "X_test_final = X_test_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0fdwarSasgT"
      },
      "source": [
        "#### 4.1 Decision Tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHxnxE0Ca0pS"
      },
      "source": [
        "A árvore de decisão é um algoritmo de aprendizado de máquina amplamente utilizado para tarefas de classificação e regressão, tomando decisões com base em um conjunto de regras e atributos de entrada. Com isso, escolha dois [parâmetros](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) para serem alterados e verifique os resultados finais. Além disso, utilize das métricas mencionadas anteriormente para verificar a eficácia do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameter 1: max_depth\n",
        "print(\"\\nParameter 1: max_depth\")\n",
        "dt_depths = [5, 10, 15, None]\n",
        "dt_depth_results = {}\n",
        "\n",
        "for depth in dt_depths:\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    dt.fit(X_train_final, y_train)\n",
        "    y_pred = dt.predict(X_test_final)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    dt_depth_results[depth] = {'accuracy': accuracy, 'f1_score': f1}\n",
        "    print(f\"max_depth={depth}: Accuracy={accuracy:.4f}, F1-Score={f1:.4f}\")\n",
        "\n",
        "# Parameter 2: min_samples_split\n",
        "print(\"\\nParameter 2: min_samples_split\")\n",
        "dt_split_values = [2, 10, 20, 50]\n",
        "dt_split_results = {}\n",
        "\n",
        "for split_val in dt_split_values:\n",
        "    dt = DecisionTreeClassifier(min_samples_split=split_val, random_state=42)\n",
        "    dt.fit(X_train_final, y_train)\n",
        "    y_pred = dt.predict(X_test_final)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    dt_split_results[split_val] = {'accuracy': accuracy, 'f1_score': f1}\n",
        "    print(f\"min_samples_split={split_val}: Accuracy={accuracy:.4f}, F1-Score={f1:.4f}\")\n",
        "\n",
        "# Best Decision Tree\n",
        "best_dt = DecisionTreeClassifier(max_depth=15, min_samples_split=10, random_state=42)\n",
        "best_dt.fit(X_train_final, y_train)\n",
        "dt_pred = best_dt.predict(X_test_final)\n",
        "\n",
        "print(f\"\\nBest Decision Tree Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, dt_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, dt_pred, average='weighted'):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, dt_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "cm_dt = confusion_matrix(y_test, dt_pred)\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Decision Tree - Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_eOQwFBb24e"
      },
      "source": [
        "#### 4.2 Ramdon Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh7oARCjb8qt"
      },
      "source": [
        "Utiliza-se o modelo 'Random Forest', já que tende a fornecer resultados mais precisos. Isso ocorre porque cada árvore é treinada em uma amostra aleatória do conjunto de dados e as previsões são feitas por consenso entre as árvores. Com isso, escolha um [parâmetro](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) para ser alterado e verifique os resultados finais. Além disso, utilize das métricas mencionadas anteriormente para verificar a eficácia do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_estimators = [50, 100, 200]\n",
        "rf_results = {}\n",
        "\n",
        "for n_est in rf_estimators:\n",
        "    rf = RandomForestClassifier(n_estimators=n_est, random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train_final, y_train)\n",
        "    y_pred = rf.predict(X_test_final)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    rf_results[n_est] = {'accuracy': accuracy, 'f1_score': f1}\n",
        "    print(f\"n_estimators={n_est}: Accuracy={accuracy:.4f}, F1-Score={f1:.4f}\")\n",
        "\n",
        "# Best Random Forest\n",
        "best_rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "best_rf.fit(X_train_final, y_train)\n",
        "rf_pred = best_rf.predict(X_test_final)\n",
        "\n",
        "print(f\"\\nBest Random Forest Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, rf_pred))\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': input_features,\n",
        "    'importance': best_rf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importance.head(10).plot(x='feature', y='importance', kind='bar')\n",
        "plt.title('Random Forest - Top 10 Feature Importances')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "cm_rf = confusion_matrix(y_test, rf_pred)\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title('Random Forest - Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYyPc-SQcqAQ"
      },
      "source": [
        "#### 4.3 KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRLUXuUXfdvx"
      },
      "source": [
        "O KNN (k-Nearest Neighbors) é um algoritmo de aprendizado supervisionado usado para classificação e regressão. Ele classifica ou prevê o valor de uma nova instância com base nos k vizinhos mais próximos no espaço de características. Com isso, escolha um [parâmetro](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer) para ser alterado e verifique os resultados finais. Além disso, utilize das métricas mencionadas anteriormente para verificar a eficácia do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_values = [3, 5, 7, 11]\n",
        "knn_results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
        "    knn.fit(X_train_final, y_train)\n",
        "    y_pred = knn.predict(X_test_final)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    knn_results[k] = {'accuracy': accuracy, 'f1_score': f1}\n",
        "    print(f\"k={k}: Accuracy={accuracy:.4f}, F1-Score={f1:.4f}\")\n",
        "\n",
        "# Best KNN\n",
        "best_knn = KNeighborsClassifier(n_neighbors=7, n_jobs=-1)\n",
        "best_knn.fit(X_train_final, y_train)\n",
        "knn_pred = best_knn.predict(X_test_final)\n",
        "\n",
        "print(f\"\\nBest KNN Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, knn_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, knn_pred, average='weighted'):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, knn_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "cm_knn = confusion_matrix(y_test, knn_pred)\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Oranges')\n",
        "plt.title('KNN - Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwgWWryQcr26"
      },
      "source": [
        "#### 4.4 Comparações finais\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWX-A61UecPc"
      },
      "source": [
        "Compare e comente sobre os três modelos feitos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results\n",
        "model_comparison = pd.DataFrame({\n",
        "    'Model': ['Decision Tree', 'Random Forest', 'KNN'],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(y_test, dt_pred),\n",
        "        accuracy_score(y_test, rf_pred),\n",
        "        accuracy_score(y_test, knn_pred)\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        f1_score(y_test, dt_pred, average='weighted'),\n",
        "        f1_score(y_test, rf_pred, average='weighted'),\n",
        "        f1_score(y_test, knn_pred, average='weighted')\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(model_comparison)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "model_comparison.set_index('Model')['Accuracy'].plot(kind='bar', ax=ax1, color='steelblue')\n",
        "ax1.set_title('Model Accuracy Comparison')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "model_comparison.set_index('Model')['F1-Score'].plot(kind='bar', ax=ax2, color='darkorange')\n",
        "ax2.set_title('Model F1-Score Comparison')\n",
        "ax2.set_ylabel('F1-Score')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Random Forest achieved the best overall performance\n",
        "- All models show good performance on this dataset\"\n",
        "- KNN shows competitive results with simpler implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFEF7cikcuhX"
      },
      "source": [
        "### 5. Aprendizagem não supervisionada\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-icvWBtcdBXr"
      },
      "source": [
        "Os métodos de aprendizagem de máquina não-supervisionados escolhidos são o `K-Means` e o `Agrupamento Hierárquico`. Para aferir a sua eficácia faz-se uso de:\n",
        "\n",
        "- **Medida de Silhueta**: Indica, de forma resumida, o quanto um indivíduo de fato se encaixa no seu cluster. Quanto mais próximo de '1' o valor de silhueta, mais similar o indivíduo é ao seu cluster, valores próximos de '0' indicam sobreposição ou ambiguidade nos agrupamentos, e valores próximos de '-1' indicam que as amostras podem ter sido atribuídas ao grupo errado.\n",
        "\n",
        "- **Índice de Calinski-Harabasz**: Avalia a separação entre os grupos formados pelo algoritmo de clustering, levando em consideração a dispersão dos pontos dentro de cada grupo e a dispersão entre os grupos. Quanto maior o valor do índice de Calinski-Harabasz, melhor é a separação dos grupos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For clustering, we'll use the scaled features without the target variable\n",
        "# Use a subset for computational efficiency\n",
        "clustering_sample_size = min(20000, len(X_train_scaled))\n",
        "sample_idx = np.random.choice(len(X_train_scaled), size=clustering_sample_size, replace=False)\n",
        "X_clustering = X_train_scaled[sample_idx]\n",
        "y_clustering = y_train.iloc[sample_idx]\n",
        "\n",
        "print(f\"Using {clustering_sample_size} samples for clustering analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAGQMcyZdbul"
      },
      "source": [
        "#### 5.1 K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBRmQp7Gd28V"
      },
      "source": [
        "K-means é um método de aprendizado de máquina não supervisionado amplamente utilizado em várias aplicações, usado para agrupar dados em clusters. O método K-Means foi escolhido por sua simplicidade e facilidade de implementação, sendo relativamente rápido e eficiente, mesmo quando aplicado a uma grande quantidade de dados. Com isso, escolha um [parâmetro](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) para ser alterado e verifique os resultados finais. Além disso, utilize das métricas mencionadas anteriormente para verificar a eficácia do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_range = range(2, 11)\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "calinski_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X_clustering)\n",
        "    \n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X_clustering, cluster_labels))\n",
        "    calinski_scores.append(calinski_harabasz_score(X_clustering, cluster_labels))\n",
        "\n",
        "# Plot elbow curve and silhouette scores\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "ax1.plot(k_range, inertias, 'bo-')\n",
        "ax1.set_xlabel('Number of Clusters (k)')\n",
        "ax1.set_ylabel('Inertia')\n",
        "ax1.set_title('K-Means: Elbow Method')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(k_range, silhouette_scores, 'ro-')\n",
        "ax2.set_xlabel('Number of Clusters (k)')\n",
        "ax2.set_ylabel('Silhouette Score')\n",
        "ax2.set_title('K-Means: Silhouette Score')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "ax3.plot(k_range, calinski_scores, 'go-')\n",
        "ax3.set_xlabel('Number of Clusters (k)')\n",
        "ax3.set_ylabel('Calinski-Harabasz Index')\n",
        "ax3.set_title('K-Means: Calinski-Harabasz Index')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal k based on silhouette score\n",
        "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
        "print(f\"Optimal number of clusters based on silhouette score: {optimal_k}\")\n",
        "\n",
        "# Test different parameters\n",
        "print(\"\\nTesting K-Means with different initialization methods:\")\n",
        "init_methods = ['k-means++', 'random']\n",
        "kmeans_results = {}\n",
        "\n",
        "for init_method in init_methods:\n",
        "    kmeans = KMeans(n_clusters=optimal_k, init=init_method, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X_clustering)\n",
        "    \n",
        "    silhouette_avg = silhouette_score(X_clustering, cluster_labels)\n",
        "    calinski_score = calinski_harabasz_score(X_clustering, cluster_labels)\n",
        "    \n",
        "    kmeans_results[init_method] = {\n",
        "        'silhouette': silhouette_avg,\n",
        "        'calinski': calinski_score\n",
        "    }\n",
        "    print(f\"init='{init_method}': Silhouette={silhouette_avg:.4f}, Calinski-Harabasz={calinski_score:.2f}\")\n",
        "\n",
        "# Best K-Means model\n",
        "best_kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)\n",
        "kmeans_labels = best_kmeans.fit_predict(X_clustering)\n",
        "\n",
        "print(f\"\\nBest K-Means Results (k={optimal_k}):\")\n",
        "print(f\"Silhouette Score: {silhouette_score(X_clustering, kmeans_labels):.4f}\")\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz_score(X_clustering, kmeans_labels):.2f}\")\n",
        "\n",
        "# Visualize K-Means clusters using PCA\n",
        "pca_cluster = PCA(n_components=2, random_state=42)\n",
        "X_pca_cluster = pca_cluster.fit_transform(X_clustering)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "scatter = plt.scatter(X_pca_cluster[:, 0], X_pca_cluster[:, 1], \n",
        "                     c=kmeans_labels, cmap='tab10', alpha=0.6, s=1)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('K-Means Clustering Results (PCA Visualization)')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "scatter = plt.scatter(X_pca_cluster[:, 0], X_pca_cluster[:, 1], \n",
        "                     c=pd.Categorical(y_clustering).codes, cmap='tab20', alpha=0.6, s=1)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('True Genre Labels (PCA Visualization)')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yNKDgKAepzO"
      },
      "source": [
        "#### 5.2 Agrupamento Hierárquico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1WIXWGlevNj"
      },
      "source": [
        "Agrupamento hierárquico é um algoritmo de aprendizado de máquina não supervisionado utilizado para agrupar dados em clusters ou grupos. Ao contrário do agrupamento k-means, que requer o número de clusters como entrada, o agrupamento hierárquico cria uma estrutura de agrupamento em forma de árvore, também conhecida como dendrograma. Com isso, escolha um [parâmetro](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn-cluster-agglomerativeclustering) para ser alterado e verifique os resultados finais. Além disso, utilize das métricas mencionadas anteriormente para verificar a eficácia do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use smaller sample for hierarchical clustering due to computational complexity\n",
        "hier_sample_size = min(5000, len(X_clustering))\n",
        "hier_sample_idx = np.random.choice(len(X_clustering), size=hier_sample_size, replace=False)\n",
        "X_hierarchical = X_clustering[hier_sample_idx]\n",
        "y_hierarchical = y_clustering.iloc[hier_sample_idx]\n",
        "\n",
        "print(f\"Using {hier_sample_size} samples for hierarchical clustering\")\n",
        "\n",
        "# Test different linkage methods\n",
        "print(\"Testing Hierarchical Clustering with different linkage methods:\")\n",
        "linkage_methods = ['ward', 'complete', 'average']\n",
        "hierarchical_results = {}\n",
        "\n",
        "for linkage in linkage_methods:\n",
        "    hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage=linkage)\n",
        "    hier_labels = hierarchical.fit_predict(X_hierarchical)\n",
        "    \n",
        "    silhouette_avg = silhouette_score(X_hierarchical, hier_labels)\n",
        "    calinski_score = calinski_harabasz_score(X_hierarchical, hier_labels)\n",
        "    \n",
        "    hierarchical_results[linkage] = {\n",
        "        'silhouette': silhouette_avg,\n",
        "        'calinski': calinski_score\n",
        "    }\n",
        "    print(f\"linkage='{linkage}': Silhouette={silhouette_avg:.4f}, Calinski-Harabasz={calinski_score:.2f}\")\n",
        "\n",
        "# Best Hierarchical model\n",
        "best_linkage = max(hierarchical_results.keys(), key=lambda x: hierarchical_results[x]['silhouette'])\n",
        "best_hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage=best_linkage)\n",
        "hierarchical_labels = best_hierarchical.fit_predict(X_hierarchical)\n",
        "\n",
        "print(f\"\\nBest Hierarchical Clustering Results (linkage='{best_linkage}'):\")\n",
        "print(f\"Silhouette Score: {silhouette_score(X_hierarchical, hierarchical_labels):.4f}\")\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz_score(X_hierarchical, hierarchical_labels):.2f}\")\n",
        "\n",
        "# Visualize Hierarchical clusters\n",
        "X_pca_hier = pca_cluster.transform(X_hierarchical)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "scatter = plt.scatter(X_pca_hier[:, 0], X_pca_hier[:, 1], \n",
        "                     c=hierarchical_labels, cmap='tab10', alpha=0.6, s=2)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('Hierarchical Clustering Results (PCA Visualization)')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "scatter = plt.scatter(X_pca_hier[:, 0], X_pca_hier[:, 1], \n",
        "                     c=pd.Categorical(y_hierarchical).codes, cmap='tab20', alpha=0.6, s=2)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('True Genre Labels (PCA Visualization)')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph-X0IQye_MK"
      },
      "source": [
        "#### 5.3 Comparações finais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kr4j8p1fDs-"
      },
      "source": [
        "Compare e comente sobre os dois modelos feitos anteriormente. Utilize gráficos para melhor visualização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare clustering methods\n",
        "clustering_comparison = pd.DataFrame({\n",
        "    'Method': ['K-Means', 'Hierarchical'],\n",
        "    'Silhouette Score': [\n",
        "        silhouette_score(X_clustering, kmeans_labels),\n",
        "        silhouette_score(X_hierarchical, hierarchical_labels)\n",
        "    ],\n",
        "    'Calinski-Harabasz Index': [\n",
        "        calinski_harabasz_score(X_clustering, kmeans_labels),\n",
        "        calinski_harabasz_score(X_hierarchical, hierarchical_labels)\n",
        "    ],\n",
        "    'Sample Size': [len(X_clustering), len(X_hierarchical)]\n",
        "})\n",
        "\n",
        "print(\"Clustering Performance Comparison:\")\n",
        "print(clustering_comparison)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "clustering_comparison.set_index('Method')['Silhouette Score'].plot(kind='bar', ax=ax1, color='purple')\n",
        "ax1.set_title('Clustering Method: Silhouette Score Comparison')\n",
        "ax1.set_ylabel('Silhouette Score')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "clustering_comparison.set_index('Method')['Calinski-Harabasz Index'].plot(kind='bar', ax=ax2, color='teal')\n",
        "ax2.set_title('Clustering Method: Calinski-Harabasz Index Comparison')\n",
        "ax2.set_ylabel('Calinski-Harabasz Index')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZC6eLucqFdo"
      },
      "source": [
        "- K-Means generally performs better in terms of computational efficiency\n",
        "- Hierarchical clustering provides more interpretable cluster structure\n",
        "- Both methods reveal natural groupings in the music feature space\n",
        "- Cluster quality metrics help validate the clustering approach"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
